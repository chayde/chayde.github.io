[ { "title": "Self-Hosted Git Repositories", "url": "/posts/self-hosted-git-repository/", "categories": "homelab", "tags": "homelab, ubuntu, bash, NetDevOps", "date": "2022-11-05 09:52:00 -0400", "snippet": "IntroI have been dipping my toe into trying to learn DevOps/NetDevOps concepts in the last few months. One of the things I’ve been curious about was learning a bit more about git and how it works, I wanted to be able to host an internal set of repositories for any projects I work on while I learn about these things. Github as a free service is great if you are ok with all of your respositories being public, while I’m learning I would greatly prefer a private respository which I’d have to pay for on Github. It’s not a huge cost but I’m trying to learn here and figuring out how to do it myself is the reason I’m working on this stuff in the first place.After a bit of research I find that it is indeed possible to internally host your own repositories and its actually faily simple to setup. I found a great write up on the Linux Foundation website that goes through the details. For this example I’m going with the bare bones Git on my own server. They have another option for installing and configuring Gitlab that offers a web based GUI similar to Github which I may try later on.Install GitThe tutorial uses a remote-server and a local-host to setup the demo and they were using Ubuntu as their Linux server of choice. The first step was to install git viasudo apt install get-coreI did not have to do this step as git was already installed in the Ubuntu 22.04 version I was running.They also suggest setting up password-less ssh login and provide the steps to do so. Again in my environment this is already setup via cloudinit whenever I clone a new VM.Setup git on remote-serverSSH to the remote server and create a project directory for git. For my examples I’m going to store things in the home directory for ‘user’ who is my lab user account. In the exmaple below you dont need to include .git in the directory nameuser@server:~ $ mkdir -p /home/user/gittest.gitNext we need to change to that directory and create an empty repocd /home/user/gittest.gitgit init --bareSetup git on local-hostNow we need to setup git on our local host, I’m using a Windows 11 workstation for this but the instructions should work the same for other operating systems.First we need to create a directory to work from then change into that directory. For my exmaple, I’m putting that on my E: drive under the repositories folder.C:\\Users\\User&gt; E:E:\\&gt; cd repositoriesE:\\repositories\\&gt; mkdir gittestE:\\repositories\\&gt; cd gittest E:\\repositories\\gittest\\&gt; Next we need to create some files for this project. I created a really basic README.md file and saved it in the directory. Then we need to initialize gitE:\\repositories\\gittest\\&gt; git init Initialized empty Git repository in E:/repositories/gittest/.git/Now we add the files from the folder into the repoE:\\repositories\\gittest\\&gt; git add .Any time you want make changes to the repo by adding/removing/modifying you will need to include a commit message describing what changed. The -a switch in the command below will apply that message to all files added with the “git add .” command above.E:\\repositories\\gittest\\&gt; git commit -m \"initial commit\" -a[master 373b548] initial commit 1 file changed, 1 insertion(+), 1 deletion(-) E:\\repositories\\gittest\\&gt; You can add a message to individual files if you specify the file name instead of using -a like this:E:\\repositories\\gittest\\&gt; git commit -m \"message\" README.md[master e517b10] message 1 file changed, 1 insertion(+)Finally we need to push these changes to the remote-server. Up to this point we have only been working on our local instance of the repository. This step will sync these changes with the remote server so you can access them from other locations. If this repository is hosted on an internet accessible server you can even collaborate with others by supplying them with the server and path information for your repository.E:\\repositories\\gittest\\&gt; git remote add origin ssh://user@remote-server/home/user/gittest.git/Now you can push or pull changes between the remote-server and the localhost using the push or pull option in gitE:\\repositories\\gittest\\&gt; git push origin masterEnumerating objects: 5, done.Counting objects: 100% (5/5), done.Delta compression using up to 16 threadsCompressing objects: 100% (2/2), done.Writing objects: 100% (3/3), 292 bytes | 292.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0), pack-reused 0To ssh://remote-server/home/user/gittest.git/ 2d1bd8a..373b548 master -&gt; masterIf you are collaborating with others or need to work on the project from another host you can clone that repo from the server with the following:git clone user@remote-server:/home/user/gittest.gitA note about vscode If you’re using vscode to as your text editor once you’ve initialized a directory for get or cloned a repository you can use the built in git feature in vscode to handle the add/commit/push/pull functions instead of having to do it manually from the cli." }, { "title": "Pihole - Forwarding Custom Domains", "url": "/posts/pihole-custom-domain-forwarding/", "categories": "homelab", "tags": "homelab, ubuntu, bash, pihole", "date": "2022-09-17 21:00:00 -0400", "snippet": "BackgroundIn my home network I have some separation for regular devices my wife and I use like our laptops/cell phones/smart tvs/etc and the devices I have in my lab environment.I’ve recently added a new network for a kubernettes testing lab I’m building. This gives me three main environments I have devices/services deployed to. Previously I used pihole installed on an Ubuntu VM for DNS/DHCP on my main network serving a DHCP addresses from a 192.168.x.x/24 pool. I used static DNS entries in the pihole server for hosts in my lab environment but this was getting annoying because of how many VMs and services I’d deployed. Additionally I wanted a separate domain for lab hosts (chayde.lab) vs hosts on my main network (chayde.lan) vs hosts in my kubernettes environment (k3.chayde.lab).SolutionTo accomplish this I need the pihole servers to forward lookups for specific domains to an alternate destinations. Pihole runs a service called dnsmasq for dns resolution. In order to accomplish what I wanted to so I deployed 2 more pihole servers one for each of the separate networks.In order to make sure any changes we made to the dnsmasq config were not over written when we perform upgrades for pihole I added a new file 03-custom-domains.conf in the /etc/dnsmasq.d/ folder.# /etc/dnsmasq.d/03-custom-domains.confserver=/k3.chayde.lab/172.20.x.xserver=/chayde.lab/172.17.x.xserver=/chayde.lan/192.168.x.x The IP addresses above are the server IPs for the pihole servers responsible for those domains/networks. I created that same file on each of the three pihole servers. I probably didn’t need have the exact same file for each server but it was easier to setup." }, { "title": "Redirect Pihole Admin Page", "url": "/posts/pihole-redirect-admin-to-root/", "categories": "homelab", "tags": "homelab, ubuntu, bash, pihole", "date": "2022-08-21 09:00:00 -0400", "snippet": "Really simple instructions today but it’s always bothered me that the admin page for configuring the pihole DNS service requires you to use http://&lt;IP/HOSTNAME&gt;/admin to reach the configuration page.I went digging through the lighttpd config file as well as executing some google-fu and found that you create a file /etc/lighttpd/external.conf which will add additional config to the process that shouldn’t be over written by the pihole update process:### Add support to redirect the default admin page from /admin to the root /url.redirect = (\"^/$\" =&gt; \"/admin\" )This line should allow you to visit the site directly http://&lt;IP/HOSTNAME&gt; with no /admin on the URL and the admin page should load.Now you just need to either reboot the host or restart the lighttpd service using: sudo systemctl restart lighttpd.service" }, { "title": "Homemade Brisket Pastrami", "url": "/posts/homemade-pastrami/", "categories": "cooking", "tags": "bbq, smoker, brisket, pastrami", "date": "2022-08-14 15:00:00 -0400", "snippet": "PastramiPastrami is one of my favorite things to eat on a sandwich when done right. Ever since getting my traeger smoker I’ve been wanting to try brining a whole brisket and then smoking it for that perfect smoked meat bite I crave. I stumbled across a video by Bradly Robinson on his Chuds BBQ youtube channel that explained the process for making home NY Style pastrami which is what I used for this recipe.The BrineFor the brine he gave the following mix in his video:1 gal Water350g Kosher Salt350 Brown Sugar42g Pink Salt10g Coriander15g Mustard Seed15g Black Peppercorn3 Bay Leaves3 Cinnamon Sticks5g Red Pepper Flakes4g Juniper Berries4g Allspice2g Cloves60g Crushed GarlicThe Rub* 2 parts pepper. * 2 parts ground coriander.* 1/2 part mustard powder* 1/2 part garlic powderThe ProcessSome Note’s I took while watching Brad’s video: Brine the brisket for 7 - 14 days in the fridge. Flip the meat occationally throughout the process to make sure you get even coverage. This tip worked out well. I did not end up with any spots on my brisket that were discolored or missing brine. When you’re ready to cook, rinse the meat off in the sink. You want to make sure all the salt, brine, spices, berries, etc are removed from the outside meat. Pat the meat dry. It doesn’t have to be bone dry but definitely use some paper towels and remove the exess moisture from the brine and rinsing process. This is also a good opportunity to do any last minute trimming before the rub. On my particular cut there were some bits of fat that had plumped up that I trimmed away so as not to have them burn off on the pit. The Rub! You can go a bit heavy on the rub if you want because there’s no salt in the rub. All the salt comes from the brining process itself. To cook I put this on the pit at 225 degrees fahrenheit, fat side up. I was using oak pellets in the smoker and Iw let it cook at that temperature until the internal temp on the flat read 170 on my probe. Once it reached 170 degrees internal I used a Foil boat (still fat side up) and I also turned the heat up to 300 degrees until it reachde an internal termperature of 200 degrees. Brad recommends a long heated rest for his smoked meats. I did this as well for this cook, over night (approximately 12 hours) in the oven at it’s lowest setting (170 degrees fahrenheit for my oven) Once the cook and rest process were completed I took the brisket and put it in the fridge to cool off completely and firm up before slicing it on my deli slicer.ResultsHonestly I was rather shocked this recipe worked the first time out. The flavor is really good. I chose to brine my brisket for 14 days. When I do this again in the future I think I’ll only do it for 5-7 instead. I feel like its a bit too salty for my taste and I’m hoping if I brine it less time it will help with that.Another take away we had was with the rub. The flavor was really nice and in the video Brad says you can go heavy and not worry about over doing it. I’d say there is definitely a way to over do it, we went extremely heavy with the rub and while it tastes good I would absolutely prefer a lot less of it on the outside. It also made slicing messy because the bark was just sliding off.All of that aside this was an excellent recipe especially for a novice like myself and the results are really delicious. " }, { "title": "Proxmox Ubuntu Cloud Init Template Notes", "url": "/posts/ubuntu-cloud-init-notes/", "categories": "homelab", "tags": "homelab, ubuntu, bash, cloud init, proxmox", "date": "2022-08-07 19:00:00 -0400", "snippet": "Ubuntu Cloud Cloud Init TeplatesAfter following all the steps to setup an Ubuntu Cloud Init template in my proxmox lab using the following: Perfect Proxmox Template with Cloud Image and Cloud InitI ran into an issue where the cloud init image creates a virtual machine with a really small virtual disk (2.2G) which was too small to be useful.According to the official Proxmox Documentation you could resize your disks while they were either online or offline with the following command from the Proxmox VE node the VM lives on:qm resize &lt;vmid&gt; &lt;disk&gt; &lt;size&gt;Example: if you wanted to add 15G to a scsi0 disk on VM 100:qm resize 100 scsi0 +15GAlternatively you can do this via the GUI by navigating to VM -&gt; Hardware -&gt; Hard Disk then click on Disk Action -&gt; Resize. Then just enter in the dialog box how many additional gigabytes you wish to increase the disk by.If you resize the disks before you boot the VM for the first time you dont have any issues. The problem I had was I’d already started a bunch of VMs with the original size, so I went searching online for a way to expand the VMs disks and found several posts about expanding the disks but you need to run several steps.I followed the steps above to resize the virtual disks. Once the disks were “physically” resized you need to make some updates in the OS so it expands the existing partitions.To start with I needed to list all the partitions:user@hostname:~$ sudo fdisk -l...Device Start End Sectors Size Type/dev/sda1 227328 40263646 40036319 2.2G Linux filesystem/dev/sda14 2048 10239 8192 4M BIOS boot/dev/sda15 10240 227327 217088 106M EFI SystemPartition table entries are not in disk order.Next I used sudo parted /dev/sda and sudo resize2fs /dev/sda1 to resize the file system then I rebooted the system to be safe.user@hostname:~$ sudo parted /dev/sdaGNU Parted 3.2Using /dev/sdaWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) print Warning: Not all of the space available to /dev/sda appears to be used, you can fix the GPT to use all of the space (an extra 1048576000blocks) or continue with the current setting?Fix/Ignore? fix Model: QEMU QEMU HARDDISK (scsi)Disk /dev/sda: 2GBSector size (logical/physical): 512B/512BPartition Table: gptDisk Flags:Number Start End Size File system Name Flags1 1049kB 2097kB 1049kB bios_grub2 2097kB 2.2GB 2.2GB ext4(parted) resizepart 2 100% Warning: Partition /dev/sda2 is being used. Are you sure you want to continue?parted: invalid token: 100% Yes/No? yes(parted) quituser@hostname:~$ sudo resize2fs /dev/sda2user@hostname:~$ sudo rebootOnce the system came back online I can see the root partition is now 20Guser@hostname:~$ df -hFilesystem Size Used Avail Use% Mounted ontmpfs 198M 964K 197M 1% /run/dev/sda1 19G 1.9G 17G 11% /tmpfs 989M 0 989M 0% /dev/shmtmpfs 5.0M 0 5.0M 0% /run/lock/dev/sda15 105M 5.3M 100M 5% /boot/efitmpfs 198M 4.0K 198M 1% /run/user/1000" }, { "title": "Proxmox Clustering Notes", "url": "/posts/proxmox-clustering/", "categories": "homelab", "tags": "homelab, proxmox, bash", "date": "2022-08-05 20:00:00 -0400", "snippet": "Creating a new ClusterCreate a cluster via the GUICreating a cluster is fairly straight forward you click on Datacenter &gt; Cluster &gt; Create Cluster then give the cluster a unique name, you cannot change this name later without destroying the cluster and starting over.Create a cluster via the CLILog into your first Proxmox node via SSH and run:pvecm create &lt;CLUSTERNAME&gt;Once the cluster has finished being created you can check the status with the following command:pvecm statusYou can list the nodes that are members of the cluster with the following command:pvecm nodesAdding Nodes to an Existing ClusterFrom the Proxmox Clustering Documentation: A node that is about to be added to the cluster cannot hold any guests. All existing configuration in /etc/pve is overwritten when joining a cluster, since guest IDs could otherwise conflict. As a workaround, you can create a backup of the guest (vzdump) and restore it under a different ID, after the node has been added to the cluster.Join a node to the cluster via the GUIJoining nodes to an existing cluster is almost as straight forward as creating the cluster was. Log into both the first node in the cluster as well as the node you want to add then navigate to click on Datacenter &gt; Cluster On the first node click on Join Information then click Copy Information Next from the node you want to add to the cluster click on Join Cluster paste the join information you copied from the first node, enter the first node’s root password in the password box and click JoinJoin a node to the cluster via the CLILog into the node you want to join to the cluster via SSH and run:pvecm add &lt;IP_FIRST_CLUSTER_NODE&gt;Removing Cluster ConfigurationsIf you have any issues with clustering or just want to completely remove the cluster configs from your nodes you will need to do the following. This is not the recommended method per the official Proxmox documentation however these were the only instructions that worked for me. Running through these steps prevented me from having to reinstall Proxmox on all my nodes. Additionally you need to ensure that all shared resources are cleanly separated or you may run into conflicts later.Stop the corosync and pve-cluster services:systemctl stop pve-clustersystemctl stop corosyncRestart the cluster file system in local mode:pmxcfs -lRemove the corosync configuration filesrm /etc/pve/corosync.confrm -r /etc/corosync/*Restart the file system again as a normal servicekillall pmxcfssystemctl start pve-clusterThe node is now separated from the cluster. You can log into the other nodes in the cluster and delete it.pvecm delnode &lt;NODE_NAME&gt;If the command fails with an error that says there was a loss of quorum you can work around that with the following commandpvecm expected 1Next switch back to the node you are removing and delete the rest of the clustering config filesrm /var/lib/corosync/*In addition to the steps above I had to also remove the entries in the /etc/pve/nodes folder. The clustering config creates a directory for each node added to the cluster.rm -rf /etc/pve/nodes/* Clustering and configuration sync happen over SSH, the nodes exchange ssh keys stored in the /etc/pve/priv/authorized_keys file, you will need to remove these entries as well.Proxmox Clustering Video" }, { "title": "Crash Course on Docker, Kubernetes, Terraform, and AWS - Part 2: Kubernetes", "url": "/posts/docker-crash-course-p2/", "categories": "homelab", "tags": "homelab, ubuntu, docker, kubernetes, aws, terraform", "date": "2022-07-31 09:00:00 -0400", "snippet": "KubernetesWhat is KubernetesKubernetes is an orchestration tool, which means it’s a tool for running and managing applications across a fleet of servers. More specifically, it’s a container orchestration tool, which means it is designed to deploy and manage applications packaged as containers (if you’re new to technologies such as containers and Docker, make sure to check out part 1 of this series, A crash course on Docker). You give Kubernetes a fleet of servers to manage and in return, it gives you all the following functionality, out-of-the-box: Scheduling: pick the optimal servers to run your containers (bin packing). Deployment: roll out changes to your containers (without downtime). Auto healing: automatically redeploy containers that failed. Auto scaling: scale the number of containers up and down with load. Networking: routing, load balancing, &amp; service discovery for containers. Configuration: configure data and secrets for containers. Data storage: manage and mount data volumes in containers.Under the hood, Kubernetes architecture consists of two main pieces: a control plane and worker nodes. Control plane: The control plane is responsible for managing the Kubernetes cluster. It is the core of the setup. The control plane is responsible for storing the state of the cluster, monitoring containers, and coordinating actions across the cluster. It runs the API server which provides an interface for various command line tools (ex: kubectl), web UIs (ex: Kubernetes Dashboard), as well as other infrastructure as code tools (ex: Terraform) to control what’s happening throughout the cluster. Worker nodes: The worker nodes are the servers the actual compute/memory/storage used to actually run your containers. The worker nodes are entirely managed by the control plane, will handle each worker node and what containers those nodes should run." }, { "title": "Crash Course on Docker, Kubernetes, Terraform, and AWS - Part 1: Docker", "url": "/posts/docker-crash-course/", "categories": "homelab", "tags": "homelab, ubuntu, docker, kubernetes, aws, terraform, IaS, infrastructure as code", "date": "2022-07-30 19:00:00 -0400", "snippet": "DockerThese notes are going to be my refrence to a blog post series a friend of mine sent me for the topic: Crash Course As I work through the posts I’ll be including relevant commands here for refrence later.Initial SetupMy setup will be using a host I created named docker01.chayde.lab (Ubuntu VM with 32g ram, 100g HDD, and 4 vCPU) for any examples I can run on local instances. Ubuntu 22.04 has been installed, initial user setup has been performed and both docker and docker-compose have been installed. I followed my previous post for installing docker and docker-composeRun Your First Docker ContainerTo run a container you will use the following command structure:$ docker run &lt;IMAGE&gt; [COMMAND]The IMAGE is the docker image to run and COMMAND are optional commands to execute with the image. For an example of how this works you can run a bash shell in an Ubuntu 20.04 Docker imageNote the command below includes the -it flag so you get an interactive shell where you can type and run other commands.$ docker run -it ubuntu:20.04 bashYou can see that you’re now inside a new container. This is a completely different environment than you were previously working from.root@e84d633423a6:/# cat /etc/os-releaseNAME=\"Ubuntu\"VERSION=\"20.04.4 LTS (Focal Fossa)\"ID=ubuntuID_LIKE=debianPRETTY_NAME=\"Ubuntu 20.04.4 LTS\"VERSION_ID=\"20.04\"HOME_URL=\"https://www.ubuntu.com/\"SUPPORT_URL=\"https://help.ubuntu.com/\"BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"VERSION_CODENAME=focalUBUNTU_CODENAME=focalRun a ls -al and you will see a list of files in the container you’re attached to. Docker images are self contained and will always run the same way for any system they run on. To see an example of this we’ll write some text into a file on this container.Use CTRL+D to exit the container.root@e84d633423a6:/# ls -altotal 56drwxr-xr-x 1 root root 4096 Jul 30 21:43 .drwxr-xr-x 1 root root 4096 Jul 30 21:43 ..-rwxr-xr-x 1 root root 0 Jul 30 21:43 .dockerenvlrwxrwxrwx 1 root root 7 May 31 15:43 bin -&gt; usr/bindrwxr-xr-x 2 root root 4096 Apr 15 2020 bootdrwxr-xr-x 5 root root 360 Jul 30 21:43 devdrwxr-xr-x 1 root root 4096 Jul 30 21:43 etcdrwxr-xr-x 2 root root 4096 Apr 15 2020 homedrwxr-xr-x 2 root root 4096 May 31 15:43 mediadrwxr-xr-x 2 root root 4096 May 31 15:43 mnt...root@e84d633423a6:/# echo \"Hello World!\" &gt; text.txtRun the same docker image again, the container should start much faster as you will not have to download the image again. Once connected run ls -al and look for your text.txt file, which you’ll notice is not there. Hit CTRL+D again and then run docker ps -a to list all of the containers on your system including ones that are stopped.$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESc85d55bb1f08 ubuntu:20.04 \"bash\" About a minute ago Exited (0) 8 seconds ago wizardly_solomone84d633423a6 ubuntu:20.04 \"bash\" 11 minutes ago Exited (0) 5 minutes ago inspiring_rideYou can start a stopped container by using docker start &lt;ID&gt; command and set the ID from the CONTAINER ID column from the docker ps command. To start the first container we created and then attach an interactive prompt:mark@docker01:~$ docker start -ia e84d633423a6root@e84d633423a6:/# lsbin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys text.txt tmp usr varroot@e84d633423a6:/# cat text.txtHello World!Each time you run docker run and the exit with CTRL+D you are leaving containers behind which take up disk space. You’ll want to make sure to clean these up with docker rm &lt;CONTAINER_ID&gt; where CONTAINER_ID is the ID from the docker ps command. Alternatively if you want the container to be removed automatically when you exit the container use the --rm flag in your docker run command.Run a web app using DockerRun a container that can be used to run a web app and try and access the web app.$ docker run training/webapp &amp; * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)$ docker run training/webapp * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)$ curl localhost:5000curl: (7) Failed to connect to localhost port 5000 after 0 ms: Connection refusedContainers are isolated not just their file system but also from a networking perspective. The container is listening on port 5000 inside the container but we need to expose that port externally as well. To do so you use the -p switch with the docker command. Hit CTRL+C to shutdown the container and run the following.$ docker run -p 5000:5000 training/webapp * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)Once it’s running you’ll be able to see the the app return results by opening a web browser to the server or using a curl command.Create A Docker Imagewe’ve used pre-built docker images so far from Docker Hub for ubuntu:20.04 and training/webapp but what about creating your own docker image? For example imagine we had a folder called web-server that contained an index.html file:&lt;html&gt; &lt;body&gt; &lt;h1&gt;Hello World from Custom Docker!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt;We could accomplish this by createing something called a dockerfile in the web-server folder with the following contents.FROM python:3WORKDIR /usr/src/appCOPY index.html .CMD [\"python\", \"-m\", \"http.server\", \"8000\"]A Dockerfile is a text file that contains a series of commands in capital letters describe to Docker how to build a Docker image. For the dockerfile we defined previously it will do the following: FROM: This specifies the base image. The preceding code uses the official python image from Docker Hub, which, as you can probably guess, has Python already installed. One convenient thing about Docker is that you can build on top of officially-maintained images that have the dependencies you need already installed. WORKDIR: This specifies the working directory for any subsequent commands. If the directory doesn’t already exist, Docker will create it. COPY: This copies files from the host OS into the Docker image. The preceding code copies the index.html file into the Docker image CMD: This specifies the default command to execute in the image when someone does docker run (if they don’t override the command). I’m using a Python command from the big list of HTTP server one-liners to fire up a simple web-server that will serve the index.html file on port 8000.To build a Docker image from your Dockerfile, go into the web-server folder, and run the docker build command:$ docker build -t example-server .Now you can run that image using the docker run command we used earlier. Make sure to include the -p switch to map port 8000 externally to the internal 8000 port$ docker run -p 8000:8000 example-serverNow if you open a web browser or curl to your http://host:8000 you’ll see our web page returned. Going forward if that was a real application you wanted to deploy you could then use the docker push command to push it to a Docker Registry (requires an authorized account).More House Cleaning CommandsTL;DR: How to Stop All Docker ContainersTo stop all Docker containers, simply run the following command in your terminal:docker kill $(docker ps -q)How It WorksThe docker ps command will list all running containers. The -q flag will only list the IDs for those containers. Once we have the list of all container IDs, we can simply run the docker kill command, passing all those IDs, and they’ll all be stopped!Remove All Docker ContainersIf you don’t just want to stop containers and you’d like to go a step further and remove them, simply run the following command:docker rm $(docker ps -a -q)How It WorksWe already know that docker ps -q will list all running container IDs. What is the -a flag? Well that will return all containers, not just the running ones. Therefore, this command will remove all containers including both running and stopped containers.How To Remove All Docker ImagesTo remove all Docker images, run this command:docker rmi $(docker images -q)###How It Worksdocker images -q will list all image IDs. We pass these IDs to docker rmi (which stands for remove images) and we therefore remove all the images.Prevent running containers from auto-restartingRational: Docker provides restart policies to control whether your containers start automatically when they exit, or when Docker restarts. This is often very useful when Docker is running a key service, this behavior however can prevent you from cleaning up old or unused containers if they’re still running.Disable ALL auto-restarting (daemon) containers.docker update --restart=no $(docker ps -a -q)Use the following to disable restart a SINGLE container.docker update --restart=no the-container-you-want-to-disable-restartNote: docker-compose restart no is the default restart policy, and it does not restart a container under any circumstance. When always is specified, the container always restarts. The on-failure policy restarts a container if the exit code indicates an on-failure error.Docker-compose restart policies:restart: \"no\"restart: alwaysrestart: on-failurerestart: unless-stopped" }, { "title": "OpenSSL Certificate Processes", "url": "/posts/certificate-processes/", "categories": "documentation", "tags": "homelab, ubuntu, bash, openssl, certificate", "date": "2022-07-28 09:10:00 -0400", "snippet": "Self Signed Certificate with SANsOpen SSL Command to runThe following command will generate a self signed certificate that will be valid for 10 years (3560 days), the key to this working is the use of an SSL options file.openssl req -x509 -days 3560 -sha256 -newkey rsa:4096 -nodes -keyout example.key -out example.crt -config v3.cnfSSL Options - v3.cnfThe contents of an example OpenSSL options file. The SAN entries are under the alt section “[alt_names]”[req]distinguished_name = req_distinguished_namex509_extensions = v3_req[req_distinguished_name]countryName = Country Name (2 letter code)countryName_default = &lt;REPLACE_WITH_TWO_LETTER_COUNTRY&gt;stateOrProvinceName = State or Province Name (full name)stateOrProvinceName_default = &lt;REPLACE_WITH_FULL_STATE_NAME&gt;localityName = Locality Name (eg, city)localityName_default = &lt;REPLACE_WITH_CITY_NAME&gt;0.organizationName = Organization Name (eg, company)0.organizationName_default = &lt;REPLACE_WITH_ORGANIZATIONAL_NAME&gt;commonName = Common NamecommonName_default = &lt;REPLACE_WITH_CERTIFICATE_CN_FQDN&gt;[ v3_req ]# Extensions to add to a certificate requestbasicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEnciphermentsubjectAltName = @alt_namesextendedKeyUsage = serverAuth, clientAuth, codeSigning, emailProtection[alt_names]DNS.1 = &lt;REPLACE_WITH_CERTIFICATE_CN_FQDN&gt;DNS.2 = &lt;REPLACE_WITH_SAN1_FQDN&gt;DNS.3 = &lt;REPLACE_WITH_SAN2_FQDN&gt;DNS.x = &lt;...&gt;" }, { "title": "ASA Capture Examples", "url": "/posts/asa-capture-example/", "categories": "documentation", "tags": "cisco, network, asa, pcap, capture, homelab", "date": "2022-07-25 07:00:00 -0400", "snippet": "ASA Capture ExamplesCapture Setup: Specify Source AND DestinationTo setup a capture on an ASA I like to create an access-list that defines the traffic I’m interested in first. For the source and destination objects you can specify single hosts or entire subnets. Obviously the more specific you can be with your capture the easier it will be to read the output created.object-group network CAP-INT network-object host &lt;SRC_IP&gt; AND/OR network-object &lt;SRC_SUBNET&gt; &lt;SRC_MASK&gt;object-group network CAP-EXT network-object host &lt;DST_IP&gt; AND/OR network-object &lt;DST_SUBNET&gt; &lt;DST_MASK&gt;access-list CAP extended permit ip object-group CAP-INT object-group CAP-EXTaccess-list CAP extended permit ip object-group CAP-EXT object-group CAP-INTCapture Setup: Specify only Source IP (or Destination)Sometimes you wont know what both IPs are, you’ll only have one IP. You can use the “any” keywords in their place.object-group network CAP-INT network-object host &lt;SRC_IP&gt; OR network-object &lt;SRC_SUBNET&gt; &lt;SRC_MASK&gt;access-list CAP extended permit ip object-group CAP-INT any4access-list CAP extended permit ip any4 object-group CAP-INTActivate the captureNext you need to start the capture and bind it to an interface. In this example “int outside” is defining the interface on my firewall I want to capture on.CAP is the name of the access list we created above.Circular-buffer will cause the ASA to constantly overwrite the buffer for the capture, dropping the oldest packet for newer once the buffer is full.Real-time will display the capture results directly to the screen as well as log them to the buffercap CAP2 int outside access-list CAP circular-buffer real-time Clean upclear cap CAP2no cap CAP2Download a copy of the capturesIf the http server is enabled on the ASA you can easily download a copy of the captures by using the following URL constructs.The /pcap at the end will format the file that is downloaded into a pcap format you can then open in Wireshark to review.SINGLE CONTEXT MODE https://&lt;ip_of_asa&gt;/admin/capture/&lt;capture_name&gt;/pcap ex: https://192.168.10.1/admin/capture/CAP/pcapMULTI CONTEXT MODE https://&lt;ip_of_asa_admin_context&gt;/admin/capture/&lt;context name&gt;/&lt;capture_name&gt;/pcap ex: https://192.168.20.1/admin/capture/ctx-prod/CAP/pcap" }, { "title": "Start ESXI VM From CLI", "url": "/posts/start-vm-from-esxi-cli/", "categories": "homelab", "tags": "homelab, esxi", "date": "2022-07-24 10:30:00 -0400", "snippet": "ESXI CLIEnable ESXI SSH access then connect the ESXi host via SSH.To get a list of all VMs run:vim-cmd vmsvc/getallvmsFind the ID for the VM you want to start and Replace &lt;##&gt; in the command below:vim-cmd vmsvc/power.on &lt;##&gt;" }, { "title": "Reset Uptimekuma Password From Docker", "url": "/posts/reset-uptimekuma-password-from-docker/", "categories": "homelab", "tags": "homelab, ubuntu, bash, docker, uptimekuma", "date": "2022-07-24 10:00:00 -0400", "snippet": "Uptimekuma Password ResetLogin into uptimekuma.chayde.lab via SSH, then run the following command:docker exec -it uptime-kuma npm run reset-password" }, { "title": "Install Docker and Docker compose", "url": "/posts/install-docker-and-docker-compose/", "categories": "homelab", "tags": "homelab, ubuntu, bash, docker, docker-compose, howto", "date": "2022-07-24 09:50:00 -0400", "snippet": "Install Dockersudo apt-get updatesudo apt-get install -y \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-releasecurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \\ \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/nullsudo apt-get updatesudo apt-get install -y docker-ce docker-ce-cli containerd.ioVerify Docker Installdocker -v Install Docker Composesudo curl -L \"https://github.com/docker/compose/releases/download/v2.8.0/docker-compose-linux-x86_64\" -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose NOTE: in the command above v2.8.0 was the latest version at the time of writting. You will need to update that value as new versions are released.Verify Docker Compose Installdocker-compose -vRun Docker Without Sudosudo usermod -aG docker $USER" }, { "title": "Change Hostname On Ubuntu Linux CLI", "url": "/posts/change-hostname-on-ubuntu/", "categories": "homelab", "tags": "homelab, ubuntu, bash", "date": "2022-07-24 09:30:00 -0400", "snippet": "Change Hostname On Ubuntu Linux CLITo view the current hostname setuphostnamectlRun command to change the hostnamesudo hostnamectl set-hostname &lt;NEW_HOSTNAME_HERE&gt;Edit the /etc/hosts file and update any localhost entriessudo nano /etc/hostsReboot the hostsudo reboot" }, { "title": "First Post - Templates", "url": "/posts/beginning/", "categories": "", "tags": "web, vscode, git, testing", "date": "2022-07-21 12:00:00 -0400", "snippet": "WelcomeHello and welcome to my homelab docs site - this page was created to use as a template of examples for various kinds of markdown and formatting I can use on the site going forward.H1 - Heading TestH2 - Heading TestH3 - Heading TestH4 - Heading TestH5 - Heading TestWhat is Lorem Ipsum?Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry’s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Why do we use it?It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using ‘Content here, content here’, making it look like readable English. Many desktop publishing packages and web page editors now use Lorem Ipsum as their default model text, and a search for ‘lorem ipsum’ will uncover many web sites still in their infancy. Various versions have evolved over the years, sometimes by accident, sometimes on purpose (injected humour and the like).Lists one BOLD TEXT ITALIC TEXT two three fourMore Lists w. colored text1) red2) blue3) greenTables Column 1 Column 2 Column 3 Value 1a Value 2a Value 3a Value 1b Value 2b Value 3b Code bock examplesJavascript code blockconsole.log('hello world!\");YML code blockname: 'push-remote'on: push: branches: - master paths-ignore: - .gitignore - README.md - LICENSEBash/Terminal/Console code blocksudo apt update &amp;&amp; sudo apt upgrade -yEmbedded tips This is an example of a Tip. This is an example of an Info block. This is an example of a Warning block. This is an example of a Danger block." } ]
